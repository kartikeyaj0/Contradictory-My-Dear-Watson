{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T06:41:31.734842Z","iopub.execute_input":"2022-05-08T06:41:31.7352Z","iopub.status.idle":"2022-05-08T06:41:31.780301Z","shell.execute_reply.started":"2022-05-08T06:41:31.735107Z","shell.execute_reply":"2022-05-08T06:41:31.779314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:31.784544Z","iopub.execute_input":"2022-05-08T06:41:31.784853Z","iopub.status.idle":"2022-05-08T06:41:44.136733Z","shell.execute_reply.started":"2022-05-08T06:41:31.784808Z","shell.execute_reply":"2022-05-08T06:41:44.13607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/contradictory-my-dear-watson/tran.csv\")\ntest_data = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:44.138343Z","iopub.execute_input":"2022-05-08T06:41:44.138611Z","iopub.status.idle":"2022-05-08T06:41:44.142744Z","shell.execute_reply.started":"2022-05-08T06:41:44.138583Z","shell.execute_reply":"2022-05-08T06:41:44.142001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case data already translated, load it directly\n# train_data = pd.read_csv(\"../input/translatedcsv/train_translated.csv\")\n# test_data = pd.read_csv(\"../input/translatedcsv/test_translated.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:44.144105Z","iopub.execute_input":"2022-05-08T06:41:44.144627Z","iopub.status.idle":"2022-05-08T06:41:44.30106Z","shell.execute_reply.started":"2022-05-08T06:41:44.144589Z","shell.execute_reply":"2022-05-08T06:41:44.300162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install googletrans==3.1.0a0\nfrom googletrans import Translator\nimport tensorflow as tf\nfrom transformers import TFAlbertModel, AlbertTokenizer, TFAutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:44.365152Z","iopub.execute_input":"2022-05-08T06:41:44.365632Z","iopub.status.idle":"2022-05-08T06:41:59.95089Z","shell.execute_reply.started":"2022-05-08T06:41:44.365602Z","shell.execute_reply":"2022-05-08T06:41:59.95007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.language.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:59.952114Z","iopub.execute_input":"2022-05-08T06:41:59.952474Z","iopub.status.idle":"2022-05-08T06:41:59.967262Z","shell.execute_reply.started":"2022-05-08T06:41:59.952445Z","shell.execute_reply":"2022-05-08T06:41:59.966492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:41:59.968645Z","iopub.execute_input":"2022-05-08T06:41:59.969055Z","iopub.status.idle":"2022-05-08T06:42:02.203583Z","shell.execute_reply.started":"2022-05-08T06:41:59.969027Z","shell.execute_reply":"2022-05-08T06:42:02.202672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load external datasets","metadata":{}},{"cell_type":"code","source":"!pip install datasets\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:02.207442Z","iopub.execute_input":"2022-05-08T06:42:02.207746Z","iopub.status.idle":"2022-05-08T06:42:10.710061Z","shell.execute_reply.started":"2022-05-08T06:42:02.207716Z","shell.execute_reply":"2022-05-08T06:42:10.70903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data[['premise', 'hypothesis', 'label']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def append_external(dataset_name):\n    train_external = []\n    dataset=load_dataset(dataset_name)\n    for record in dataset['train']:\n        c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n        train_external.append((c1, c2, c3))\n\n    train_external = pd.DataFrame(train_external, columns=['premise', 'hypothesis', 'label'])\n    train_data = pd.concat([train_data, train_external])\n    return train_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = append_external('xnli')\n# train_data = append_external('multi_nli')\n# train_data = append_external('snli')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.535027Z","iopub.execute_input":"2022-05-08T06:42:53.535591Z","iopub.status.idle":"2022-05-08T06:42:53.548394Z","shell.execute_reply.started":"2022-05-08T06:42:53.535549Z","shell.execute_reply":"2022-05-08T06:42:53.547855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translate using the Google Translate API\n\nSince alBERT is pretrained on only English language dataset, we translate all the data into English","metadata":{}},{"cell_type":"code","source":"train_not_eng = train_data[train_data.lang_abv != 'en']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.549516Z","iopub.execute_input":"2022-05-08T06:42:53.54973Z","iopub.status.idle":"2022-05-08T06:42:53.560635Z","shell.execute_reply.started":"2022-05-08T06:42:53.549705Z","shell.execute_reply":"2022-05-08T06:42:53.559734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_eng = train_data[train_data.lang_abv == 'en']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.561943Z","iopub.execute_input":"2022-05-08T06:42:53.562606Z","iopub.status.idle":"2022-05-08T06:42:53.571264Z","shell.execute_reply.started":"2022-05-08T06:42:53.56257Z","shell.execute_reply":"2022-05-08T06:42:53.570268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _to_en(text):\n    translator = Translator()\n    decoded = translator.translate(text, dest='en').text\n    return decoded","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.572408Z","iopub.execute_input":"2022-05-08T06:42:53.573327Z","iopub.status.idle":"2022-05-08T06:42:53.582223Z","shell.execute_reply.started":"2022-05-08T06:42:53.573294Z","shell.execute_reply":"2022-05-08T06:42:53.581601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_not_eng = train_data[train_data.lang_abv != 'en'].copy()\ntest_data_not_eng = test_data[test_data.lang_abv != 'en'].copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.583595Z","iopub.execute_input":"2022-05-08T06:42:53.58401Z","iopub.status.idle":"2022-05-08T06:42:53.594218Z","shell.execute_reply.started":"2022-05-08T06:42:53.58398Z","shell.execute_reply":"2022-05-08T06:42:53.593553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n# started at 11:39am\n\ntrain_data_not_eng.premise = train_data_not_eng.premise.apply(to_english)\ntrain_data_not_eng.hypothesis = train_data_not_eng.hypothesis.apply(to_english)\ntest_data_not_eng.premise = test_data_not_eng.premise.apply(to_english)\ntest_data_not_eng.hypothesis = test_data_not_eng.hypothesis.apply(to_english)\n\n# print time taken for translation\nprint(time.time()-start_time)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.595428Z","iopub.execute_input":"2022-05-08T06:42:53.595691Z","iopub.status.idle":"2022-05-08T06:42:53.605909Z","shell.execute_reply.started":"2022-05-08T06:42:53.595662Z","shell.execute_reply":"2022-05-08T06:42:53.605277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add back the translated data into the train and test datasets","metadata":{}},{"cell_type":"code","source":"train_data[train_data.lang_abv != 'en'] = train_data_not_eng\ntest_data[test_data.lang_abv != 'en'] = test_data_not_eng","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.60726Z","iopub.execute_input":"2022-05-08T06:42:53.607835Z","iopub.status.idle":"2022-05-08T06:42:53.617171Z","shell.execute_reply.started":"2022-05-08T06:42:53.607781Z","shell.execute_reply":"2022-05-08T06:42:53.616542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the translations for future use","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open('train_data.pickle', 'wb') as f:\n    pickle.dump(train_data, f)\n    \nwith open('test_data.pickle', 'wb') as f:\n    pickle.dump(test_data, f)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.63106Z","iopub.execute_input":"2022-05-08T06:42:53.631386Z","iopub.status.idle":"2022-05-08T06:42:53.641708Z","shell.execute_reply.started":"2022-05-08T06:42:53.631359Z","shell.execute_reply":"2022-05-08T06:42:53.64086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode the text","metadata":{}},{"cell_type":"code","source":"SEQ_LEN=256\ndef albert_encode(df, tokenizer):\n    batch_premises = df['premise'].tolist()\n    batch_hypothesis = df['hypothesis'].tolist()\n\n    tokens = tokenizer(batch_premises, batch_hypothesis, max_length = SEQ_LEN,\n                   truncation=True, padding='max_length',\n                   add_special_tokens=True, return_attention_mask=True,\n                   return_token_type_ids=True,\n                   return_tensors='tf')\n    inputs = {\n          'input_ids': tokens['input_ids'], \n          'attention_mask': tokens['attention_mask'],\n          'token_type_ids': tokens['token_type_ids']  }  \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.643061Z","iopub.execute_input":"2022-05-08T06:42:53.643434Z","iopub.status.idle":"2022-05-08T06:42:53.653888Z","shell.execute_reply.started":"2022-05-08T06:42:53.643329Z","shell.execute_reply":"2022-05-08T06:42:53.653026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = albert_encode(train_data, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:42:53.655281Z","iopub.execute_input":"2022-05-08T06:42:53.656034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(train_input['input_ids'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the ML model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\ndef build_model(): \n   \n    input_ids = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids = tf.keras.Input(shape=(SEQ_LEN,), \n                                    dtype=tf.int32,  name=\"token_type_ids\")\n        \n    model = TFAutoModel.from_pretrained(\"albert-base-v2\")\n    embedding = model([input_ids, attention_mask , token_type_ids])[0] \n    inputs=[input_ids, attention_mask  , token_type_ids ] \n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n        \n    final_model = tf.keras.Model(inputs=inputs, outputs=output)\n    hp_learning_rate = 1e-6\n    final_model.compile(tf.keras.optimizers.Adam(lr = hp_learning_rate),\n                        loss='sparse_categorical_crossentropy', metrics=['accuracy'])   \n    return final_model ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): \n    albert_model = build_model()\n    albert_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.label.values.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"albert_model.fit(train_input, train_data.label.values,\n                 epochs = 5, verbose = 1, batch_size = 30, validation_split = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a submission file","metadata":{}},{"cell_type":"code","source":"test_input = albert_encode(test_data, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in albert_model.predict(test_input)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_data.id.copy().to_frame()\nsubmission['prediction'] = predictions\n\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}